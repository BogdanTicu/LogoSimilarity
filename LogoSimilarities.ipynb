{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFleOHvuUFImvuEqeX9X/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BogdanTicu/LogoSimilarity/blob/main/LogoSimilarities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYsTjdPGBc_c",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import concurrent.futures\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from PIL import Image\n",
        "import io\n",
        "import hdbscan\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from sklearn.decomposition import PCA\n",
        "#folosim modelul ResNet50\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "\n",
        "def download_logo(url):\n",
        "    try: #folosind requests incerc sa accesez un url, daca reusesc fac conversie a imaginii in RGB\n",
        "        response = requests.get(url, stream=True, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return Image.open(response.raw).convert('RGB')\n",
        "    except requests.RequestException:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "#Descarcam imaginile in paralel pe 10 threaduri pt optimizare\n",
        "def download_logos_parallel(domains):\n",
        "    urls = [f\"https://logo.clearbit.com/{d}\" for d in domains]\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        images = list(executor.map(download_logo, urls))\n",
        "\n",
        "  #iau fiecare domeniu si imagine, si daca am gasit imagini valide atunci le pun in vectorul valid_data sub forma de perechi de elemente, domeniu, imagine\n",
        "  #daca nu am imagine valida, le pun in \"skipped_domains\"(l-am folosit pentru a face debug).\n",
        "    valid_data = []\n",
        "    skipped_domains = []\n",
        "    for d,img in zip(domains, images):\n",
        "      if img is not None:\n",
        "        valid_data.append((d,img))\n",
        "      else:\n",
        "        skipped_domains.append(d)\n",
        "\n",
        "    #print(f\"Număr de logo-uri descărcate: {len(valid_data)}\")\n",
        "    #print(f\"Domenii fără logo: {len(skipped_domains)}\")\n",
        "    #print(skipped_domains)\n",
        "\n",
        "    return valid_data\n",
        "\n",
        "#transform imaginile intr-un array pentru a le procesa modelul ResNet50\n",
        "def extract_features_batch(images):\n",
        "    img_arrays = np.vstack([\n",
        "        preprocess_input(np.expand_dims(image.img_to_array(img.resize((224, 224))), axis=0))\n",
        "        for img in images\n",
        "    ])\n",
        "    return model.predict(img_arrays)\n",
        "\n",
        "#Creez un minhash pentru fiecare vector de features.\n",
        "def compute_minhash(feature_vector, num_perm=128):\n",
        "    minhash = MinHash(num_perm=num_perm)\n",
        "    for val in feature_vector[:300]:\n",
        "        minhash.update(str(val).encode('utf8'))\n",
        "    return minhash\n",
        "\n",
        "#Citesc primele 1000 domenii(pentru testare mai rapida) si le pun intr-o lista.\n",
        "df = pd.read_parquet(\"logos.snappy.parquet\").head(1000)\n",
        "domains = df['domain'].tolist()\n",
        "\n",
        "#descarc logourile\n",
        "valid_data = download_logos_parallel(domains)\n",
        "\n",
        "if not valid_data:\n",
        "    print(\"Eroare: Nu s-au descărcat logo-uri valide!\")\n",
        "    exit()\n",
        "\n",
        "valid_domains, images = zip(*valid_data)\n",
        "\n",
        "#Extrage caracteristicile imaginilor\n",
        "features_list = extract_features_batch(images)\n",
        "\n",
        "#folosim lsh pentru a gasi mai rapid grupurile asemanatoare.\n",
        "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
        "hashes = {str(i): compute_minhash(features_list[i]) for i in range(len(features_list))}\n",
        "for i, h in hashes.items():\n",
        "    lsh.insert(i, h)\n",
        "\n",
        "#cream grupuri cu lsh\n",
        "clusters = {}\n",
        "for i in range(len(features_list)):\n",
        "    similar_logos = lsh.query(hashes[str(i)])\n",
        "    cluster_id = tuple(sorted(similar_logos))\n",
        "    clusters[cluster_id] = clusters.get(cluster_id, []) + [valid_domains[i]]\n",
        "\n",
        "#transformam grupurile in array-uri ca sa aplicam hdbscan pentru a grupa logourile.\n",
        "cluster_vectors = np.array([features_list[int(i)] for i in hashes.keys()])\n",
        "hdbscan_cluster = hdbscan.HDBSCAN(metric='euclidean', min_cluster_size=2)\n",
        "labels = hdbscan_cluster.fit_predict(cluster_vectors)\n",
        "\n",
        "#afisam grupurile.\n",
        "final_clusters = {}\n",
        "for idx, label in enumerate(labels):\n",
        "    if label != -1:\n",
        "        final_clusters.setdefault(label, []).append(valid_domains[idx])\n",
        "    else:\n",
        "        # Daca este noise (label == -1), adaugă domeniul la grupul \"Noise\".\n",
        "        final_clusters.setdefault('Noise', []).append(valid_domains[idx])\n",
        "\n",
        "for cluster_id, domains in final_clusters.items():\n",
        "    print(f\"Group {cluster_id}: {domains}\")\n"
      ]
    }
  ]
}